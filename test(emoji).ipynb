{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cf443a5-4e9d-4d48-aacf-74325d4b5fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fce3b7e0-e5c6-46d4-bc3d-18186795be56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27544 images belonging to 7 classes.\n",
      "Epoch 1/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 85ms/step - accuracy: 0.2338 - loss: 1.8295 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 78ms/step - accuracy: 0.2608 - loss: 1.7997 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 56ms/step - accuracy: 0.2664 - loss: 1.7822 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 52ms/step - accuracy: 0.2649 - loss: 1.7818 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 73ms/step - accuracy: 0.2706 - loss: 1.7769 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 53ms/step - accuracy: 0.2708 - loss: 1.7687 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 143ms/step - accuracy: 0.2758 - loss: 1.7611 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 43ms/step - accuracy: 0.2800 - loss: 1.7559 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 45ms/step - accuracy: 0.2831 - loss: 1.7488 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 109ms/step - accuracy: 0.2930 - loss: 1.7432 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 40ms/step - accuracy: 0.2915 - loss: 1.7387 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 47ms/step - accuracy: 0.3002 - loss: 1.7196 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 46ms/step - accuracy: 0.3073 - loss: 1.7077 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 46ms/step - accuracy: 0.3168 - loss: 1.6958 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 48ms/step - accuracy: 0.3226 - loss: 1.6843 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 47ms/step - accuracy: 0.3324 - loss: 1.6645 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 51ms/step - accuracy: 0.3300 - loss: 1.6641 - learning_rate: 1.0000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 78ms/step - accuracy: 0.3317 - loss: 1.6521 - learning_rate: 1.0000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 52ms/step - accuracy: 0.3474 - loss: 1.6401 - learning_rate: 1.0000e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 49ms/step - accuracy: 0.3463 - loss: 1.6334 - learning_rate: 1.0000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 46ms/step - accuracy: 0.3452 - loss: 1.6175 - learning_rate: 1.0000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 44ms/step - accuracy: 0.3483 - loss: 1.6218 - learning_rate: 1.0000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 45ms/step - accuracy: 0.3541 - loss: 1.6094 - learning_rate: 1.0000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 44ms/step - accuracy: 0.3544 - loss: 1.5990 - learning_rate: 1.0000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 46ms/step - accuracy: 0.3636 - loss: 1.5903 - learning_rate: 1.0000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 49ms/step - accuracy: 0.3625 - loss: 1.5857 - learning_rate: 1.0000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 47ms/step - accuracy: 0.3710 - loss: 1.5800 - learning_rate: 1.0000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 89ms/step - accuracy: 0.3703 - loss: 1.5700 - learning_rate: 1.0000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 125ms/step - accuracy: 0.3795 - loss: 1.5667 - learning_rate: 1.0000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 86ms/step - accuracy: 0.3779 - loss: 1.5585 - learning_rate: 1.0000e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 55ms/step - accuracy: 0.3754 - loss: 1.5491 - learning_rate: 1.0000e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 45ms/step - accuracy: 0.3888 - loss: 1.5414 - learning_rate: 1.0000e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 44ms/step - accuracy: 0.3877 - loss: 1.5314 - learning_rate: 1.0000e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 43ms/step - accuracy: 0.3853 - loss: 1.5325 - learning_rate: 1.0000e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 43ms/step - accuracy: 0.3864 - loss: 1.5313 - learning_rate: 1.0000e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 42ms/step - accuracy: 0.3908 - loss: 1.5223 - learning_rate: 1.0000e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 45ms/step - accuracy: 0.3934 - loss: 1.5163 - learning_rate: 1.0000e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m880s\u001b[0m 1s/step - accuracy: 0.3961 - loss: 1.5085 - learning_rate: 1.0000e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 87ms/step - accuracy: 0.4035 - loss: 1.4970 - learning_rate: 1.0000e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m955s\u001b[0m 1s/step - accuracy: 0.3968 - loss: 1.5073 - learning_rate: 1.0000e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 80ms/step - accuracy: 0.4002 - loss: 1.5003 - learning_rate: 1.0000e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 38ms/step - accuracy: 0.4110 - loss: 1.4796 - learning_rate: 1.0000e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 37ms/step - accuracy: 0.4035 - loss: 1.4889 - learning_rate: 1.0000e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 40ms/step - accuracy: 0.4022 - loss: 1.4889 - learning_rate: 1.0000e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 44ms/step - accuracy: 0.4082 - loss: 1.4784 - learning_rate: 1.0000e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 47ms/step - accuracy: 0.4115 - loss: 1.4771 - learning_rate: 1.0000e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 47ms/step - accuracy: 0.4117 - loss: 1.4657 - learning_rate: 1.0000e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 47ms/step - accuracy: 0.4142 - loss: 1.4706 - learning_rate: 1.0000e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m748/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m6:08\u001b[0m 3s/step - accuracy: 0.4169 - loss: 1.4599"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tkinter import Tk, Button, Label\n",
    "from PIL import Image as PILImage, ImageTk\n",
    "\n",
    "# Step 1: Define your model architecture with more layers and dropout to prevent overfitting\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First Conv Layer\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    # Second Conv Layer\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    # Third Conv Layer\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    # Flatten the output of the final Conv layer\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Fully connected layers with dropout for regularization\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))  # Dropout to prevent overfitting\n",
    "    model.add(Dense(7, activation='softmax'))  # Output layer for 7 emotions\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Step 2: Data Augmentation for improved generalization\n",
    "train_data_dir = r'C:\\Users\\anshu\\Emoji-Generator\\data\\train'  # Update with your data path\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(48, 48),\n",
    "    color_mode='grayscale',\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Step 3: Create and train the model\n",
    "emotion_model = create_model()\n",
    "\n",
    "# Learning Rate Scheduler to reduce the learning rate as training progresses\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Train the model\n",
    "emotion_model.fit(train_generator, epochs=100, callbacks=[reduce_lr])  # Adjust the number of epochs as needed\n",
    "\n",
    "# Save the model weights\n",
    "emotion_model.save_weights('emotion_model_final.weights.h5')\n",
    "\n",
    "# Step 4: Load the model weights (for later use)\n",
    "def load_model():\n",
    "    model = create_model()\n",
    "    model.load_weights('emotion_model_final.weights.h5')\n",
    "    return model\n",
    "\n",
    "# Step 5: Capture an image from the webcam and predict the emotion\n",
    "def capture_and_predict(model):\n",
    "    emoji_dict = {\n",
    "        0: cv2.imread(r'C:\\Users\\anshu\\Emoji-Generator\\emojis\\Angry.png', -1),\n",
    "        1: cv2.imread(r'C:\\Users\\anshu\\Emoji-Generator\\emojis\\disgusted.png', -1),\n",
    "        2: cv2.imread(r'C:\\Users\\anshu\\Emoji-Generator\\emojis\\happy.png', -1),\n",
    "        3: cv2.imread(r'C:\\Users\\anshu\\Emoji-Generator\\emojis\\sad.png', -1),\n",
    "        4: cv2.imread(r'C:\\Users\\anshu\\Emoji-Generator\\emojis\\surprised.png', -1),\n",
    "        5: cv2.imread(r'C:\\Users\\anshu\\Emoji-Generator\\emojis\\neutral.png', -1),\n",
    "        6: cv2.imread(r'C:\\Users\\anshu\\Emoji-Generator\\emojis\\fearful.png', -1)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edd2c923-814f-423f-a918-ecc9bcbced83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'c' to capture an image\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "Predicted Emotion Index: 6\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Function to display the image and emoji side by side\n",
    "def display_image_with_emoji(captured_image, emoji):\n",
    "    # Convert the emoji from RGBA (with alpha) to RGB by removing the alpha channel\n",
    "    if emoji.shape[2] == 4:\n",
    "        emoji = cv2.cvtColor(emoji, cv2.COLOR_BGRA2BGR)\n",
    "\n",
    "    # Resize emoji to match the captured image height\n",
    "    emoji_resized = cv2.resize(emoji, (captured_image.shape[1], captured_image.shape[0]))\n",
    "    \n",
    "    # Combine the two images horizontally\n",
    "    combined_image = np.hstack((captured_image, emoji_resized))\n",
    "    \n",
    "    # Show the combined image\n",
    "    cv2.imshow('Captured Image and Emoji', combined_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Open webcam to capture an image\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"Press 'c' to capture an image\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    cv2.imshow('Webcam', frame)\n",
    "    \n",
    "    key = cv2.waitKey(1)\n",
    "    if key & 0xFF == ord('c'):\n",
    "        # Capture the image\n",
    "        cv2.imwrite('captured_image.jpg', frame)\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Process the image for prediction\n",
    "img = cv2.imread('captured_image.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "img = cv2.resize(img, (48, 48))\n",
    "img = img.astype('float32') / 255.0\n",
    "img = np.reshape(img, (1, 48, 48, 1))\n",
    "\n",
    "# Predict the emotion using the loaded model\n",
    "prediction = loaded_model.predict(img)\n",
    "emotion_index = np.argmax(prediction)\n",
    "print(f'Predicted Emotion Index: {emotion_index}')\n",
    "\n",
    "# Display the emoji corresponding to the predicted emotion\n",
    "emoji = emoji_dict[emotion_index]\n",
    "\n",
    "# Display the captured image and emoji side by side\n",
    "captured_image = cv2.imread('captured_image.jpg')  # Load the original colored captured image\n",
    "display_image_with_emoji(captured_image, emoji)\n",
    "\n",
    "# Main flow\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the model (this will load the weights saved earlier)\n",
    "    loaded_model = load_model()\n",
    "    # Capture an image and predict the emotion\n",
    "    capture_and_predict(loaded_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d3e6e9-f633-470e-9d6c-88ee00671b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0efd4d-adf9-4d79-b54d-707880fa2305",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
